---
layout: post
title: كيف تستخدم بايثون للتنقيب في البيانات في صفحات الويب؟
---

تمتاز بايثون بقدرتها العالية في كل مكان ، فهي لغة مرنة سهلة التركيب والبنية وتقريبًا يمكن إستخدامها لأي شيء ، واحدة من أكثر المجالات رواجًا اليوم هو مجال التنقيب عن البيانات أو data mining  ، بايثون تحتوي على مجموعة مميزة من المكتبات التي تستخدم للتنقيب عن البيانات ، اليوم سأحاول في هذا المقال شرح مكتبة Beautiful Soup (حساء جميل )  وأخذ لمحة شمولية سريعة عن تنقيب البيانات وكيفية إستخدام هذه المكتبة بأكثر طريقة فاعلة .

### ماهو تنقيب البيانات على الويب؟
الشبكة العنكبوتية تحتوي على كم هائل جدا من  البيانات ، في تقدير يوجد اليوم على الشبكة ما يزيد عن 1.2 مليون تيرابايت من البيانات ! ، هذا الكم الهائل من البيانات قد يبدوا لك منظمًا ومتاحا للوصول أليه في أي لحظة من خلال محركات البحث المشهورة ، لكن يجب عليك أن تعيد التفكير مرة أخرى ، 99 % من هذه البيانات لا يمكنك الوصول إليها عبر الطرق التقليدية مثل محركات البحث  ، 1% من هذه البيانات تمت فهرستها لدى هذه المحركات .
إذا ، لنفترض أنك تود الحصول على معلومة معينة ، أو تود عمل إحصائية عن مدى إستخدام مصطلح معين عبر مجموعة كبيرة من صفحات الويب ، هل ستقوم بذلك يدويًا ؟

الحقيقة نعم يمكنك فعل هذا اﻷمر يدويًا  ، لكن تخيل معي مدى الصعوبة التي ستواجهك وكمية الوقت الذي ستمضيه وانت تبحث أو ( تنقب )  في هذه البيانات ؟ ساعات عديدة اليس كذلك ؟!

لدى ويكيبديا تعريف جيد للتنقيب عن البيانات وهو :
 >التنقيب في البيانات(بالانجليزية: data mining) هي عملية بحث محوسب ويدوي عن معرفة من البيانات دون فرضيات مسبقة عما يمكن أن تكون هذه المعرفة.
 
 Beautiful Soup هي مكتبة مميزة مبنية على بايثون ، يوجد منها نسخ عديدة لمنصات مختلفة مثل C#  ، بشكل عام تقوم مكتبة BS  بالحصول على البيانات من صفحات الويب المكتوبة بلغة HTML  و XML  ، مما يعني سحب البيانات المعروضة والمتاحة للعامة على صفحات الويب .
 
 تم تطوير هذه المكتبة عبر Leonard Richardson عام 2004 ، سنستخدم طرفية بايثون للتنصيب  .
 
### تنصيب BS 4 
عملية التنصيب سهلة ، يوجد طريقتان ، اﻷولى لمستخدمي لينوكس وسنستخدم فيها مثبت الحزمة الإفتراضي الذي يأتي مع لينوكس  وهو apt 
ببساطة قم بكتابة اﻷمر التالي في الطرفية  في حال كنت تستخدم بايثون الإصدار 2
```
apt-get install python-bs4
```

الإصدار الثالث أكتب :

```
apt-get install python3-bs4
```
أما الطريقة الثانية فهي تصلح لجميع المستخدمين ، وسنستخدم فيها مثبت حزم بايثون والمعروف إختصار بـ PIP  وذلك عبر الأمر التالي 
```
pip install beautifulsoup4
```

يفترض الآن أن BS أصبحت مثبتة لدينا ،إذا لننتقل إلى المسلمات في هذه المكتبة  !

### المسلمات 
لنفترض أن لدينا صفحة HTML محتواها كالتالي :
```
<!DOCTYPE html>
<html>
<head>
<title>website zero - zero contnet !</title>
</head>
<body>
<a href="/about">link</a>
<a href="/how">link</a>
<a href="/faq">link</a>
<a href="/call">link</a>
<a href="/help">link</a>

<p>Ipsum </p>
</body>
</html>
```
سنعمل في البداية على تخزين هذه الصفحة داخل متغير وتعريفه كمستند HTML في مكتبة BS
شاهد الكود التالي :

```python
from bs4 import BeautifulSoup # إستدعاء المكتبة
# سنقوم بتخزين مستند HTML كقيمة نصية في متغير myHTML
myHTML = """
	<!DOCTYPE html>
	<html>
	<head>
		<title>website zero - zero contnet !</title>
	</head>
	<body>
	<a href="/about">link</a>
	<a href="/how">link</a>
	<a href="/faq">link</a>
	<a href="/call">link</a>
	<a href="/help">link</a>

	<p>Ipsum </p>
	</body>
	</html>
"""
#  الآن سنقوم بإعلام BS  بمتغيرنا الجديد 
soup = BeautifulSoup(myHTML, 'html.parser') 

# لنجرب طباعة هذا المستند
print(soup.prettify())

```
المخرجات ستكون كالتالي :


```
<!DOCTYPE html>
<html>
 <head>
  <title>
   website zero - zero contnet !
  </title>
 </head>
 <body>
  <a href="/about">
   link
  </a>
  <a href="/how">
   link
  </a>
  <a href="/faq">
   link
  </a>
  <a href="/call">
   link
  </a>
  <a href="/help">
   link
  </a>
  <p>
   Ipsum
  </p>
 </body>
</html>

```
لاحظ أنه أصبح أكثر تنسيقًا وجمالًا عن المدخلات  ، وهذا ما تقوم به prettify() .

لنجرب الآن سحب كافة الروابط من مستند HTML الخاص بنا  ، BS  توفر العديد من الخيارات المخصصة لسحب أي نوع من الوسوم أو البيانات ، كما يمكنك إستخدام تعبير من نوعregular ex.  

شاهد المثال التالي  :
```python
for link in soup.find_all('a'):
    print(link.get('href'))
```

المخرجات ستكون قيمة العنصر herf داخل كل وسم a
```
/about
/how
/faq
/call
/help
```
يمكن كذلك إستخدام find_all()لوحدها .


هذه لمحة بسيطة ومقدمة لسلسلة من المواضيع الأخرى في هذه المكتبة  ، في المقال القادم سنناقش كيفية إستخدام urllib  للوصول إلى محتوى صفحات موجودة على  شبكة الإنترنت
 
